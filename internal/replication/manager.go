package replication

import (
	"maps"
	"slices"
	"sync"
	"time"

	"github.com/minhvip08/simis/internal/config"
	"github.com/minhvip08/simis/internal/connection"
	"github.com/minhvip08/simis/internal/utils"
)

type Manager struct {
	replicas map[*connection.RedisConnection]*ReplicaInfo // Map of replica ID -> ReplicaInfo
	mu       sync.RWMutex                                 // Protects concurrent access
}

var (
	manager *Manager
	once    sync.Once
)

// GetManager creates a new replication manager
func GetManager() *Manager {
	once.Do(func() {
		manager = &Manager{
			replicas: make(map[*connection.RedisConnection]*ReplicaInfo),
		}
	})
	return manager
}

// AddReplica adds a new replica with the given ID
func (m *Manager) AddReplica(replicaInfo *ReplicaInfo) {
	m.mu.Lock()
	defer m.mu.Unlock()

	m.replicas[replicaInfo.Connection] = replicaInfo
}

// GetAllReplicas returns all replicas
func (m *Manager) GetAllReplicas() []*ReplicaInfo {
	m.mu.RLock()
	defer m.mu.RUnlock()
	return slices.Collect(maps.Values(m.replicas))
}

// GetReplica returns the replica info for the given connection
func (m *Manager) GetReplica(conn *connection.RedisConnection) *ReplicaInfo {
	m.mu.RLock()
	defer m.mu.RUnlock()
	return m.replicas[conn]
}

// GetReplicaCount returns the number of connected replicas
func (m *Manager) GetReplicaCount() int {
	m.mu.RLock()
	defer m.mu.RUnlock()
	return len(m.replicas)
}

// PropagateCommand sends a command to all connected replicas
func (m *Manager) PropagateCommand(cmdName string, args []string) {
	if config.GetInstance().Role == config.Slave {
		return
	}

	m.mu.RLock()
	defer m.mu.RUnlock()

	// Build RESP array format for the command
	cmdArray := []string{cmdName}
	cmdArray = append(cmdArray, args...)
	respCommand := utils.ToRespArray(cmdArray)

	// Send to all connected replicas
	eligibleReplicas := m.getEligibleReplicas()
	for _, replica := range eligibleReplicas {
		replica.Connection.SendResponse(respCommand)
	}

	// Increment the master offset
	config.GetInstance().AddOffset(len(respCommand))
}

func (m *Manager) WaitForReplicas(minReplicasToWaitFor int, timeout int) int {
	// If no replicas (who have completed the handshake) are available, return 0
	// If no replicas are needed to be waited for, return 0
	if minReplicasToWaitFor == 0 || len(m.getEligibleReplicas()) == 0 {
		return 0
	}

	currentOffset := config.GetInstance().GetOffset()
	if currentOffset == 0 {
		return m.GetReplicaCount()
	}
	deadline := time.Now().Add(time.Duration(timeout) * time.Millisecond)

	for {
		updatedReplicas := m.sendGetAckToReplicas(currentOffset)

		if updatedReplicas >= minReplicasToWaitFor {
			return updatedReplicas
		}

		if timeout > 0 && time.Now().After(deadline) {
			return updatedReplicas
		}

		// Small sleep to avoid busy loop; ACKs are processed in the session goroutine
		time.Sleep(10 * time.Millisecond)
	}
}

func (m *Manager) getEligibleReplicas() []*ReplicaInfo {
	eligibleReplicas := make([]*ReplicaInfo, 0)
	for _, replica := range m.replicas {
		if replica.IsHandshakeDone {
			eligibleReplicas = append(eligibleReplicas, replica)
		}
	}
	return eligibleReplicas
}

// sendGetAckToReplicas: ask replicas for ACK, then count those whose LastAckOffset
// has caught up to our current offset.
func (m *Manager) sendGetAckToReplicas(targetOffset int) int {
	m.PropagateCommand("REPLCONF", []string{"GETACK", "*"})

	// Give replicas a brief moment to respond and for the session loop to parse + update LastAckOffset.
	time.Sleep(10 * time.Millisecond)

	updatedReplicas := 0
	for _, replica := range m.GetAllReplicas() {
		if replica.LastAckOffset >= targetOffset {
			updatedReplicas++
		}
	}

	return updatedReplicas
}
